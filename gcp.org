#+TITLE:       GCP Notes       
#+AUTHOR:      Richard Wilson
#+DATE:        5/5/21

#+OPTIONS: ^:{}
#+OPTIONS: todo:nil

* Storage
** Create Bucket
#+begin_src bash
gsutil mb -p PROJECT_ID -c STORAGE_CLASS -l BUCKET_LOCATION -b on gs://BUCKET_NAME
#+end_src
** Storage Class
STANDARD  - frequently accessed or only brief storage
NEARLINE  - read or modify once per month or less
COLDLINE  - read or modify once per quarter or less
ARCHIVE   - read or modify once per year or less
** File Metadata
#+begin_src python
from google.cloud import storage


def blob_metadata(bucket_name, blob_name):
    """Prints out a blob's metadata."""
    # bucket_name = 'your-bucket-name'
    # blob_name = 'your-object-name'

    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)

    # Retrieve a blob, and its metadata, from Google Cloud Storage.
    # Note that `get_blob` differs from `Bucket.blob`, which does not
    # make an HTTP request.
    blob = bucket.get_blob(blob_name)

    print("Blob: {}".format(blob.name))
    print("Bucket: {}".format(blob.bucket.name))
    print("Storage class: {}".format(blob.storage_class))
    print("ID: {}".format(blob.id))
    print("Size: {} bytes".format(blob.size))
    print("Updated: {}".format(blob.updated))
    print("Generation: {}".format(blob.generation))
    print("Metageneration: {}".format(blob.metageneration))
    print("Etag: {}".format(blob.etag))
    print("Owner: {}".format(blob.owner))
    print("Component count: {}".format(blob.component_count))
    print("Crc32c: {}".format(blob.crc32c))
    print("md5_hash: {}".format(blob.md5_hash))
    print("Cache-control: {}".format(blob.cache_control))
    print("Content-type: {}".format(blob.content_type))
    print("Content-disposition: {}".format(blob.content_disposition))
    print("Content-encoding: {}".format(blob.content_encoding))
    print("Content-language: {}".format(blob.content_language))
    print("Metadata: {}".format(blob.metadata))
    print("Custom Time: {}".format(blob.custom_time))
    print("Temporary hold: ", "enabled" if blob.temporary_hold else "disabled")
    print(
        "Event based hold: ",
        "enabled" if blob.event_based_hold else "disabled",
    )
    if blob.retention_expiration_time:
        print(
            "retentionExpirationTime: {}".format(
                blob.retention_expiration_time
            )
        )

#+end_src
* BigQuuery
** Table Exists
#+begin_src python
from google.cloud import bigquery
from google.cloud.exceptions import NotFound

client = bigquery.Client()

# TODO(developer): Set table_id to the ID of the table to determine existence.
# table_id = "your-project.your_dataset.your_table"

try:
    client.get_table(table_id)  # Make an API request.
    print("Table {} already exists.".format(table_id))
except NotFound:
    print("Table {} is not found.".format(table_id))
#+end_src
** Copy Table
#+begin_src python
copy_job = bqclient.copy_table(TEMP_TABLE_ID, TABLE_ID)
print("Table " + TABLE_ID + " does not exist, performing copy operation")
copy_job.result()  # Wait for the job to complete.
print('Copied from {} into {} in {}'.format(TEMP_TABLE_ID, TABLE_ID, str(copy_job.ended - copy_job.started)) )
#+end_src
** Check all columns for null
#+begin_src sql
SELECT col_name, COUNT(1) nulls_count
FROM `project.dataset.table` t,
UNNEST(REGEXP_EXTRACT_ALL(TO_JSON_STRING(t), r'"(\w+)":null')) col_name
GROUP BY col_name 
#+end_src
** Delimited column header
#+begin_src python
col_query = '''
select 
 col.column_name
from 
 `{0}.{1}.INFORMATION_SCHEMA.COLUMNS` col
where 
 col.table_name = '{2}'
order by 
 col.column_name
'''.format(PROJECT_ID, DATASET_ID, INPUT_TABLE_NAME)

COLUMN_LIST = pandas_gbq.read_gbq(col_query,project_id=PROJECT_ID,use_bqstorage_api=True)

HEADER_LIST = COLUMN_LIST.astype(str).values.flatten().tolist()
HEADER_STRING = '|'.join(HEADER_LIST)
print(HEADER_STRING)
#+end_src
* Instances
** Poweroff from Python
#+begin_src python
os.system("sudo poweroff")
#+end_src

* Dataflow
** SQL to TXT
#+begin_src python

from __future__ import division
import apache_beam as beam
from apache_beam.io import ReadFromBigQuery, WriteToText
from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions, WorkerOptions
import logging
######################################
#        Pipeline Configuration      #
######################################
#Set options for pipeline
pipeline_options = PipelineOptions(
      runner='DataflowRunner',
      project='PROJECT_ID',
      job_name='JOBNAME',
      region='us-east4',
      subnetwork= 'SUBNETWORK',
      use_public_ips=False,
      staging_location = 'gs://STAGING_LOCATION/',
      temp_location = 'gs://TEMP_LOCATION',
      machine_type = "n1-standard-1",
      max_num_workers = 30,
      autoscaling_algorithm='THROUGHPUT_BASED'#None
     )

#Set options for the file to be exported
DESTINATION_FILE_CONFIG = {
    'file_path_prefix' : 'gs://PATH_HERE'
    ,'file_name_suffix' : '.txt'
    ,'num_shards' : 1 # number of files to be created
    ,'header' : HEADER_STRING
   }


#ParDo class
class JoiningDoFn2(beam.DoFn):
    # Do lazy initializaiton here. Otherwise, error messages pop up, associated with "A large DoFn instance that is serialized for transmission to remote workers.""
    def __init__(self):
        import pandas as pd
	self.pd = pd
    def process(self,dic):
       return ['|'.join(str(x) for x in dic.values())]

 ######################################
 #               Pipeline             #
 ######################################

class DataFlowPipeline:
    """THIS IS THE CLASS THAT ACTUALLY RUNS THE JOB"""

    def run(self):
        """This is the job runner it holds the beam pipeline"""
        with beam.Pipeline(options=pipeline_options) as p:
	    pull_table_query =  """
	     select *
	     from 
	     {0}        
	    """.format(INPUT_TABLE_ID)

	#Driver averaging pipeline
	ent_modeling = p | 'read table' >> beam.io.Read(beam.io.ReadFromBigQuery(query=pull_table_query, use_standard_sql=True))  \
	                 | 'ParDo' >> beam.ParDo(JoiningDoFn2())  \
		         | 'Write Result to file' >> beam.io.WriteToText(**DESTINATION_FILE_CONFIG)

######################################
#              Main                  #
######################################
if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)
    print('setting up config for runner...')
    trainer = DataFlowPipeline()
    trainer.run()
    print('The runner is done!')
#+end_src

* Airflow Composer
** Example from Cloud Composer:Qwik Start
#+begin_src python
# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Example Airflow DAG that checks if a local file exists, creates a Cloud Dataproc cluster, runs the Hadoop
wordcount example, and deletes the cluster.
This DAG relies on three Airflow variables
https://airflow.apache.org/concepts.html#variables
* gcp_project - Google Cloud Project to use for the Cloud Dataproc cluster.
* gce_zone - Google Compute Engine zone where Cloud Dataproc cluster should be
  created.
* gcs_bucket - Google Cloud Storage bucket to use for result of Hadoop job.
  See https://cloud.google.com/storage/docs/creating-buckets for creating a
  bucket.
"""
import datetime
import os
from airflow import models
from airflow.contrib.operators import dataproc_operator
from airflow.operators import BashOperator
from airflow.utils import trigger_rule
# Output file for Cloud Dataproc job.
output_file = os.path.join(
    models.Variable.get('gcs_bucket'), 'wordcount',
    datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) + os.sep
# Path to Hadoop wordcount example available on every Dataproc cluster.
WORDCOUNT_JAR = (
    'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
)
# Path to input file for Hadoop job.
input_file = '/home/airflow/gcs/data/rose.txt'
# Arguments to pass to Cloud Dataproc job.
wordcount_args = ['wordcount', input_file, output_file]
yesterday = datetime.datetime.combine(
    datetime.datetime.today() - datetime.timedelta(1),
    datetime.datetime.min.time())
default_dag_args = {
    # Setting start date as yesterday starts the DAG immediately when it is
    # detected in the Cloud Storage bucket.
    'start_date': yesterday,
    # To email on failure or retry set 'email' arg to your email and enable
    # emailing here.
    'email_on_failure': False,
    'email_on_retry': False,
    # If a task fails, retry it once after waiting at least 5 minutes
    'retries': 1,
    'retry_delay': datetime.timedelta(minutes=5),
    'project_id': models.Variable.get('gcp_project')
}
with models.DAG(
        'Composer_sample_quickstart',
        # Continue to run DAG once per day
        schedule_interval=datetime.timedelta(days=1),
        default_args=default_dag_args) as dag:
    # Check if the input file exists.
    check_file_existence =  BashOperator(
        task_id='check_file_existence',
        bash_command='if [ ! -f \"{}\" ]; then exit 1;  fi'.format(input_file))
   # Create a Cloud Dataproc cluster.
    create_dataproc_cluster = dataproc_operator.DataprocClusterCreateOperator(
        task_id='create_dataproc_cluster',
        # Give the cluster a unique name by appending the date scheduled.
        # See https://airflow.apache.org/code.html#default-variables
        cluster_name='quickstart-cluster-{{ ds_nodash }}',
        num_workers=2,
        image_version='2.0',
        zone=models.Variable.get('gce_zone'),
        region='us-central1',
        master_machine_type='n1-standard-2',
        worker_machine_type='n1-standard-2')
   # Run the Hadoop wordcount example installed on the Cloud Dataproc cluster
    # master node.
    run_dataproc_hadoop = dataproc_operator.DataProcHadoopOperator(
        task_id='run_dataproc_hadoop',
        region='us-central1',
        main_jar=WORDCOUNT_JAR,
        cluster_name='quickstart-cluster-{{ ds_nodash }}',
        arguments=wordcount_args)
   # Delete Cloud Dataproc cluster.
    delete_dataproc_cluster = dataproc_operator.DataprocClusterDeleteOperator(
        task_id='delete_dataproc_cluster',
        cluster_name='quickstart-cluster-{{ ds_nodash }}',
        region='us-central1',
        # Setting trigger_rule to ALL_DONE causes the cluster to be deleted
        # even if the Dataproc job fails.
        trigger_rule=trigger_rule.TriggerRule.ALL_DONE)
   # Define DAG dependencies.
    check_file_existence >> create_dataproc_cluster >> run_dataproc_hadoop >> delete_dataproc_cluster
#+end_src
