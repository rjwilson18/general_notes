#+TITLE:       GCP Notes       
#+AUTHOR:      Richard Wilson
#+DATE:        5/5/21

#+OPTIONS: ^:{}
#+OPTIONS: todo:nil

* Storage
** Create Bucket
#+begin_src bash
gsutil mb -p PROJECT_ID -c STORAGE_CLASS -l BUCKET_LOCATION -b on gs://BUCKET_NAME
#+end_src
** Storage Class
STANDARD  - frequently accessed or only brief storage
NEARLINE  - read or modify once per month or less
COLDLINE  - read or modify once per quarter or less
ARCHIVE   - read or modify once per year or less
** File Metadata
#+begin_src python
from google.cloud import storage


def blob_metadata(bucket_name, blob_name):
    """Prints out a blob's metadata."""
    # bucket_name = 'your-bucket-name'
    # blob_name = 'your-object-name'

    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)

    # Retrieve a blob, and its metadata, from Google Cloud Storage.
    # Note that `get_blob` differs from `Bucket.blob`, which does not
    # make an HTTP request.
    blob = bucket.get_blob(blob_name)

    print("Blob: {}".format(blob.name))
    print("Bucket: {}".format(blob.bucket.name))
    print("Storage class: {}".format(blob.storage_class))
    print("ID: {}".format(blob.id))
    print("Size: {} bytes".format(blob.size))
    print("Updated: {}".format(blob.updated))
    print("Generation: {}".format(blob.generation))
    print("Metageneration: {}".format(blob.metageneration))
    print("Etag: {}".format(blob.etag))
    print("Owner: {}".format(blob.owner))
    print("Component count: {}".format(blob.component_count))
    print("Crc32c: {}".format(blob.crc32c))
    print("md5_hash: {}".format(blob.md5_hash))
    print("Cache-control: {}".format(blob.cache_control))
    print("Content-type: {}".format(blob.content_type))
    print("Content-disposition: {}".format(blob.content_disposition))
    print("Content-encoding: {}".format(blob.content_encoding))
    print("Content-language: {}".format(blob.content_language))
    print("Metadata: {}".format(blob.metadata))
    print("Custom Time: {}".format(blob.custom_time))
    print("Temporary hold: ", "enabled" if blob.temporary_hold else "disabled")
    print(
        "Event based hold: ",
        "enabled" if blob.event_based_hold else "disabled",
    )
    if blob.retention_expiration_time:
        print(
            "retentionExpirationTime: {}".format(
                blob.retention_expiration_time
            )
        )

#+end_src
** FIle Movement
*** Large files Upload
This command uses the gsutil to upload files in parallel.
This can be used on one large file or a group of files.

Arguments: //
-n prevents overwriting existing files //
-m performs parallel multi-threading/processing for more than 1 file.
-o GSUtil:parallel_composite_upload_threshold=150M breaks large files into chunks of declared size and uploads them in parallel.
#+begin_src bash
gsutil -o GSUtil:parallel_composite_upload_threshold=150M -m cp -n src gs://dest
#+end_src

or in Python
#+begin_src python
filename='myfile.csv'
gs_bucket='my/bucket'
parallel_threshold='150M' # minimum size for parallel upload; 0 to disable

subprocess.check_call([
  'gsutil',
  '-o', 'GSUtil:parallel_composite_upload_threshold=%s' % (parallel_threshold,),
  'cp', filename, 'gs://%s/%s' % (gs_bucket, filename)
])
#+end_src
* BigQuery
** Dataset Exists
#+begin_src python
dataset = bq.Dataset(DATASET_ID)

try:
    dataset = bqclient.create_dataset(dataset)
except:
    print("Dataset " + DATASET_ID + " already exists")
#+end_src
** Table Exists
#+begin_src python
from google.cloud import bigquery
from google.cloud.exceptions import NotFound

client = bigquery.Client()

# TODO(developer): Set table_id to the ID of the table to determine existence.
# table_id = "your-project.your_dataset.your_table"

try:
    client.get_table(table_id)  # Make an API request.
    print("Table {} already exists.".format(table_id))
except NotFound:
    print("Table {} is not found.".format(table_id))
#+end_src
** Copy Table
#+begin_src python
copy_job = bqclient.copy_table(TEMP_TABLE_ID, TABLE_ID)
print("Table " + TABLE_ID + " does not exist, performing copy operation")
copy_job.result()  # Wait for the job to complete.
print('Copied from {} into {} in {}'.format(TEMP_TABLE_ID, TABLE_ID, str(copy_job.ended - copy_job.started)) )
#+end_src
** Snapshot Table
Requires IAM Permissions: bigquery.tables.createSnapshot, bigquery.tables.get, bigquery.tables.getData

#+begin_src sql
create snapshot table `SNAPSHOT_TABLE_ID`
clone `TABLE_ID`;
#+end_src
** Check all columns for null
#+begin_src sql
SELECT col_name, COUNT(1) nulls_count
FROM `project.dataset.table` t,
UNNEST(REGEXP_EXTRACT_ALL(TO_JSON_STRING(t), r'"(\w+)":null')) col_name
GROUP BY col_name 
#+end_src
** Delimited column header
#+begin_src python
col_query = '''
select 
 col.column_name
from 
 `{0}.{1}.INFORMATION_SCHEMA.COLUMNS` col
where 
 col.table_name = '{2}'
order by 
 col.column_name
'''.format(PROJECT_ID, DATASET_ID, INPUT_TABLE_NAME)

COLUMN_LIST = pandas_gbq.read_gbq(col_query,project_id=PROJECT_ID,use_bqstorage_api=True)

HEADER_LIST = COLUMN_LIST.astype(str).values.flatten().tolist()
HEADER_STRING = '|'.join(HEADER_LIST)
print(HEADER_STRING)
#+end_src
** Dedupe Table
*** Method 1
#+begin_src python
#Check for duplicates
query = '''
select 
 count(1) as row_count
 ,count(distinct *) as distinct_row_count
from 
 `{0}` 
'''.format(TABLE_ID)

# run query using panda_gbq, project_id is the location of the tools project.  data location is set in the query string
query_results = pandas_gbq.read_gbq(query,project_id=GCP_PROJECT,use_bqstorage_api=True)
print(query_results)

if ~np.where(query_results['row_count'] == query_results['distinct_row_count'],True,False):
    print('duplicates found. deduping....')
    dedupe_query = '''
     create or replace table `{0}` as 
     select distinct * from `{0}`; 
    '''.format(TABLE_ID)
    
    #dedupe then rerun dupe check
    dedupe_query_job = bqclient.query(dedupe_query)
    results = dedupe_query_job.result()
    query_results = pandas_gbq.read_gbq(query,project_id=GCP_PROJECT,use_bqstorage_api=True)
    print(query_results)
#+end_src
*** Method 2
This method is faster and more memory conscious so it is better for larger tables.
It can also better incorporate a where clause for partial deduping.
#+begin_src sql
MERGE `transactions.testdata` t
USING (
  SELECT DISTINCT *
  FROM `transactions.testdata`
)
ON FALSE
WHEN NOT MATCHED BY SOURCE THEN DELETE
WHEN NOT MATCHED BY TARGET THEN INSERT ROW
#+end_src
** Table Metadata
#+begin_src sql
  select 
   t.project_id
   ,t.dataset_id
   ,table_id
   ,timestamp_millis(t.creation_time) as creation_time
   ,timestamp_millis(t.last_modified_time) as last_modified_time
   ,t.row_count
   ,t.size_bytes * .000000000931 as size_gb
   ,case when type = 1 then 'table'
	 when type = 2 then 'view'
	 else null end as type
  from 
   `TABLE_ID`.__TABLES__ t

#+end_src
** Shell Commands
| Function       | Command                                                                                                                                    |
|----------------+--------------------------------------------------------------------------------------------------------------------------------------------|
| Create Dataset | gcloud alpha bq datasets create <dataset-name>  --description '<description>' --if-exists 'overwrite'                                      |
| List Datasets  | gcloud alpha bq datasets list --all                                                                                                        |
| Delete Dataset | gcloud alpha bq datasets delete <dataset-name>                                                                                             |
| Copy Table     | gcloud alpha bq tables copy --source <table_name1> --destination <table_name2 --source-dataset <dataset1> --destination-dataset <dataset2> |
| Delete Table   | gcloud alpha bq tables delete <table-name>  --dataset=<dataset-name>                                                                       |
| Preview Table  | gcloud alpha bq tables show-rows --table <table_name> --limit <x> --start <y>                                                              |
| List Tables    | gcloud alpha bq tables list --dataset <dataset-name>                                                                                       |
* Instances
** Poweroff from Python
#+begin_src python
os.system("sudo poweroff")
#+end_src
* Dataflow
** SQL to TXT
#+begin_src python

from __future__ import division
import apache_beam as beam
from apache_beam.io import ReadFromBigQuery, WriteToText
from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions, WorkerOptions
import logging
######################################
#        Pipeline Configuration      #
######################################
#Set options for pipeline
pipeline_options = PipelineOptions(
      runner='DataflowRunner',
      project='PROJECT_ID',
      job_name='JOBNAME',
      region='us-east4',
      subnetwork= 'SUBNETWORK',
      use_public_ips=False,
      staging_location = 'gs://STAGING_LOCATION/',
      temp_location = 'gs://TEMP_LOCATION',
      machine_type = "n1-standard-1",
      max_num_workers = 30,
      autoscaling_algorithm='THROUGHPUT_BASED'#None
     )

#Set options for the file to be exported
DESTINATION_FILE_CONFIG = {
    'file_path_prefix' : 'gs://PATH_HERE'
    ,'file_name_suffix' : '.txt'
    ,'num_shards' : 1 # number of files to be created
    ,'header' : HEADER_STRING
   }


#ParDo class
class JoiningDoFn2(beam.DoFn):
    # Do lazy initializaiton here. Otherwise, error messages pop up, associated with "A large DoFn instance that is serialized for transmission to remote workers.""
    def __init__(self):
        import pandas as pd
	self.pd = pd
    def process(self,dic):
       return ['|'.join(str(x) for x in dic.values())]

 ######################################
 #               Pipeline             #
 ######################################

class DataFlowPipeline:
    """THIS IS THE CLASS THAT ACTUALLY RUNS THE JOB"""

    def run(self):
        """This is the job runner it holds the beam pipeline"""
        with beam.Pipeline(options=pipeline_options) as p:
	    pull_table_query =  """
	     select *
	     from 
	     {0}        
	    """.format(INPUT_TABLE_ID)

	#Driver averaging pipeline
	ent_modeling = p | 'read table' >> beam.io.Read(beam.io.ReadFromBigQuery(query=pull_table_query, use_standard_sql=True))  \
	                 | 'ParDo' >> beam.ParDo(JoiningDoFn2())  \
		         | 'Write Result to file' >> beam.io.WriteToText(**DESTINATION_FILE_CONFIG)

######################################
#              Main                  #
######################################
if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)
    print('setting up config for runner...')
    trainer = DataFlowPipeline()
    trainer.run()
    print('The runner is done!')
#+end_src

* Airflow Composer
** Example from Cloud Composer:Qwik Start
#+begin_src python
"""Example Airflow DAG that checks if a local file exists, creates a Cloud Dataproc cluster, runs the Hadoop
wordcount example, and deletes the cluster.
This DAG relies on three Airflow variables
https://airflow.apache.org/concepts.html#variables
 gcp_project - Google Cloud Project to use for the Cloud Dataproc cluster.
 gce_zone - Google Compute Engine zone where Cloud Dataproc cluster should be
 created.
 gcs_bucket - Google Cloud Storage bucket to use for result of Hadoop job.
  See https://cloud.google.com/storage/docs/creating-buckets for creating a
  bucket.
"""
import datetime
import os
from airflow import models
from airflow.contrib.operators import dataproc_operator
from airflow.operators import BashOperator
from airflow.utils import trigger_rule
# Output file for Cloud Dataproc job.
output_file = os.path.join(
    models.Variable.get('gcs_bucket'), 'wordcount',
    datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) + os.sep
# Path to Hadoop wordcount example available on every Dataproc cluster.
WORDCOUNT_JAR = (
    'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
)
# Path to input file for Hadoop job.
input_file = '/home/airflow/gcs/data/rose.txt'
# Arguments to pass to Cloud Dataproc job.
wordcount_args = ['wordcount', input_file, output_file]
yesterday = datetime.datetime.combine(
    datetime.datetime.today() - datetime.timedelta(1),
    datetime.datetime.min.time())
default_dag_args = {
    # Setting start date as yesterday starts the DAG immediately when it is
    # detected in the Cloud Storage bucket.
    'start_date': yesterday,
    # To email on failure or retry set 'email' arg to your email and enable
    # emailing here.
    'email_on_failure': False,
    'email_on_retry': False,
    # If a task fails, retry it once after waiting at least 5 minutes
    'retries': 1,
    'retry_delay': datetime.timedelta(minutes=5),
    'project_id': models.Variable.get('gcp_project')
}
with models.DAG(
        'Composer_sample_quickstart',
        # Continue to run DAG once per day
        schedule_interval=datetime.timedelta(days=1),
        default_args=default_dag_args) as dag:
    # Check if the input file exists.
    check_file_existence =  BashOperator(
        task_id='check_file_existence',
        bash_command='if [ ! -f \"{}\" ]; then exit 1;  fi'.format(input_file))
   # Create a Cloud Dataproc cluster.
    create_dataproc_cluster = dataproc_operator.DataprocClusterCreateOperator(
        task_id='create_dataproc_cluster',
        # Give the cluster a unique name by appending the date scheduled.
        # See https://airflow.apache.org/code.html#default-variables
        cluster_name='quickstart-cluster-{{ ds_nodash }}',
        num_workers=2,
        image_version='2.0',
        zone=models.Variable.get('gce_zone'),
        region='us-central1',
        master_machine_type='n1-standard-2',
        worker_machine_type='n1-standard-2')
   # Run the Hadoop wordcount example installed on the Cloud Dataproc cluster
    # master node.
    run_dataproc_hadoop = dataproc_operator.DataProcHadoopOperator(
        task_id='run_dataproc_hadoop',
        region='us-central1',
        main_jar=WORDCOUNT_JAR,
        cluster_name='quickstart-cluster-{{ ds_nodash }}',
        arguments=wordcount_args)
   # Delete Cloud Dataproc cluster.
    delete_dataproc_cluster = dataproc_operator.DataprocClusterDeleteOperator(
        task_id='delete_dataproc_cluster',
        cluster_name='quickstart-cluster-{{ ds_nodash }}',
        region='us-central1',
        # Setting trigger_rule to ALL_DONE causes the cluster to be deleted
        # even if the Dataproc job fails.
        trigger_rule=trigger_rule.TriggerRule.ALL_DONE)
   # Define DAG dependencies.
    check_file_existence >> create_dataproc_cluster >> run_dataproc_hadoop >> delete_dataproc_cluster
#+end_src


* PubSub
** Example from Stream Processing with Cloud Pub/Sub and Dataflow: Qwik Start
*** Setup
Define Constants in cloud shell
#+begin_src bash
PROJECT_ID=$(gcloud config get-value project)
BUCKET_NAME=$PROJECT_ID
TOPIC_ID=my-id
REGION=us-central1
#+end_src

Create a Cloud Storage bucket owned by this project:
#+begin_src bash
gsutil mb gs://$BUCKET_NAME
#+end_src

Create a Pub/Sub topic in this project:
#+begin_src bash
gcloud pubsub topics create $TOPIC_ID
#+end_src

Create a Cloud Scheduler job in this project. The job publishes a message to a Pub/Sub topic at one-minute intervals.

If an App Engine app does not exist for the project, this step will create one.
#+begin_src bash
gcloud scheduler jobs create pubsub publisher-job --schedule="* * * * *" \
    --topic=$TOPIC_ID --message-body="Hello!"
#+end_src
If prompted to enable the Cloud Scheduler API, press y and enter.

If prompted to create an App Engine app, press y and select us-central for its region.

*** Git repo and setup
#+begin_src bash
virtualenv env
source env/bin/activate
git clone https://github.com/GoogleCloudPlatform/python-docs-samples.git
cd python-docs-samples/pubsub/streaming-analytics
pip install -U -r requirements.txt  # Install Apache Beam dependencies
#+end_src

*** Dataflow code
#+begin_src python
import argparse
from datetime import datetime
import logging
import random
from apache_beam import DoFn, GroupByKey, io, ParDo, Pipeline, PTransform, WindowInto, WithKeys
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.transforms.window import FixedWindows
class GroupMessagesByFixedWindows(PTransform):
    """A composite transform that groups Pub/Sub messages based on publish time
    and outputs a list of tuples, each containing a message and its publish time.
    """
    def __init__(self, window_size, num_shards=5):
        # Set window size to 60 seconds.
        self.window_size = int(window_size * 60)
        self.num_shards = num_shards
    def expand(self, pcoll):
        return (
            pcoll
            # Bind window info to each element using element timestamp (or publish time).
            | "Window into fixed intervals"
            >> WindowInto(FixedWindows(self.window_size))
            | "Add timestamp to windowed elements" >> ParDo(AddTimestamp())
            # Assign a random key to each windowed element based on the number of shards.
            | "Add key" >> WithKeys(lambda _: random.randint(0, self.num_shards - 1))
            # Group windowed elements by key. All the elements in the same window must fit
            # memory for this. If not, you need to use `beam.util.BatchElements`.
            | "Group by key" >> GroupByKey()
        )
class AddTimestamp(DoFn):
    def process(self, element, publish_time=DoFn.TimestampParam):
        """Processes each windowed element by extracting the message body and its
        publish time into a tuple.
        """
        yield (
            element.decode("utf-8"),
            datetime.utcfromtimestamp(float(publish_time)).strftime(
                "%Y-%m-%d %H:%M:%S.%f"
            ),
        )
class WriteToGCS(DoFn):
    def __init__(self, output_path):
        self.output_path = output_path
    def process(self, key_value, window=DoFn.WindowParam):
        """Write messages in a batch to Google Cloud Storage."""
        ts_format = "%H:%M"
        window_start = window.start.to_utc_datetime().strftime(ts_format)
        window_end = window.end.to_utc_datetime().strftime(ts_format)
        shard_id, batch = key_value
        filename = "-".join([self.output_path, window_start, window_end, str(shard_id)])
        with io.gcsio.GcsIO().open(filename=filename, mode="w") as f:
            for message_body, publish_time in batch:
                f.write(f"{message_body},{publish_time}\n".encode("utf-8"))
def run(input_topic, output_path, window_size=1.0, num_shards=5, pipeline_args=None):
    # Set `save_main_session` to True so DoFns can access globally imported modules.
    pipeline_options = PipelineOptions(
        pipeline_args, streaming=True, save_main_session=True
    )
    with Pipeline(options=pipeline_options) as pipeline:
        (
            pipeline
            # Because `timestamp_attribute` is unspecified in `ReadFromPubSub`, Beam
            # binds the publish time returned by the Pub/Sub server for each message
            # to the element's timestamp parameter, accessible via `DoFn.TimestampParam`.
            # https://beam.apache.org/releases/pydoc/current/apache_beam.io.gcp.pubsub.html#apache_beam.io.gcp.pubsub.ReadFromPubSub
            | "Read from Pub/Sub" >> io.ReadFromPubSub(topic=input_topic)
            | "Window into" >> GroupMessagesByFixedWindows(window_size, num_shards)
            | "Write to GCS" >> ParDo(WriteToGCS(output_path))
        )
if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input_topic",
        help="The Cloud Pub/Sub topic to read from."
        '"projects//topics/".',
    )
    parser.add_argument(
        "--window_size",
        type=float,
        default=1.0,
        help="Output file's window size in minutes.",
    )
    parser.add_argument(
        "--output_path",
        help="Path of the output GCS file including the prefix.",
    )
    parser.add_argument(
        "--num_shards",
        type=int,
        default=5,
        help="Number of shards to use when writing windowed elements to GCS.",
    )
    known_args, pipeline_args = parser.parse_known_args()
    run(
        known_args.input_topic,
        known_args.output_path,
        known_args.window_size,
        known_args.num_shards,
        pipeline_args,
    )
#+end_src

*** Run Command
#+begin_src bash
python PubSubToGCS.py \
    --project=$PROJECT_ID \
    --region=$REGION \
    --input_topic=projects/$PROJECT_ID/topics/$TOPIC_ID \
    --output_path=gs://$BUCKET_NAME/samples/output \
    --runner=DataflowRunner \
    --window_size=2 \
    --num_shards=2 \
    --temp_location=gs://$BUCKET_NAME/temp
#+end_src

*** Cleanup
#+begin_src 
gcloud scheduler jobs delete publisher-job
gcloud pubsub topics delete $TOPIC_ID
gsutil -m rm -rf "gs://${BUCKET_NAME}/samples/output*"
gsutil -m rm -rf "gs://${BUCKET_NAME}/temp/*"
gsutil rb gs://${BUCKET_NAME}
#+end_src

