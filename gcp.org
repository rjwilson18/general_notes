#+TITLE:       GCP Notes       
#+AUTHOR:      Richard Wilson
#+DATE:        5/5/21

#+OPTIONS: ^:{}
#+OPTIONS: todo:nil

* Storage
** Create Bucket
#+begin_src bash
gsutil mb -p PROJECT_ID -c STORAGE_CLASS -l BUCKET_LOCATION -b on gs://BUCKET_NAME
#+end_src
** Storage Class
STANDARD  - frequently accessed or only brief storage
NEARLINE  - read or modify once per month or less
COLDLINE  - read or modify once per quarter or less
ARCHIVE   - read or modify once per year or less
** File Metadata
#+begin_src python
from google.cloud import storage


def blob_metadata(bucket_name, blob_name):
    """Prints out a blob's metadata."""
    # bucket_name = 'your-bucket-name'
    # blob_name = 'your-object-name'

    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)

    # Retrieve a blob, and its metadata, from Google Cloud Storage.
    # Note that `get_blob` differs from `Bucket.blob`, which does not
    # make an HTTP request.
    blob = bucket.get_blob(blob_name)

    print("Blob: {}".format(blob.name))
    print("Bucket: {}".format(blob.bucket.name))
    print("Storage class: {}".format(blob.storage_class))
    print("ID: {}".format(blob.id))
    print("Size: {} bytes".format(blob.size))
    print("Updated: {}".format(blob.updated))
    print("Generation: {}".format(blob.generation))
    print("Metageneration: {}".format(blob.metageneration))
    print("Etag: {}".format(blob.etag))
    print("Owner: {}".format(blob.owner))
    print("Component count: {}".format(blob.component_count))
    print("Crc32c: {}".format(blob.crc32c))
    print("md5_hash: {}".format(blob.md5_hash))
    print("Cache-control: {}".format(blob.cache_control))
    print("Content-type: {}".format(blob.content_type))
    print("Content-disposition: {}".format(blob.content_disposition))
    print("Content-encoding: {}".format(blob.content_encoding))
    print("Content-language: {}".format(blob.content_language))
    print("Metadata: {}".format(blob.metadata))
    print("Custom Time: {}".format(blob.custom_time))
    print("Temporary hold: ", "enabled" if blob.temporary_hold else "disabled")
    print(
        "Event based hold: ",
        "enabled" if blob.event_based_hold else "disabled",
    )
    if blob.retention_expiration_time:
        print(
            "retentionExpirationTime: {}".format(
                blob.retention_expiration_time
            )
        )

#+end_src
* BigQuuery
** Table Exists
#+begin_src python
from google.cloud import bigquery
from google.cloud.exceptions import NotFound

client = bigquery.Client()

# TODO(developer): Set table_id to the ID of the table to determine existence.
# table_id = "your-project.your_dataset.your_table"

try:
    client.get_table(table_id)  # Make an API request.
    print("Table {} already exists.".format(table_id))
except NotFound:
    print("Table {} is not found.".format(table_id))
#+end_src
** Copy Table
#+begin_src python
copy_job = bqclient.copy_table(TEMP_TABLE_ID, TABLE_ID)
print("Table " + TABLE_ID + " does not exist, performing copy operation")
copy_job.result()  # Wait for the job to complete.
print('Copied from {} into {} in {}'.format(TEMP_TABLE_ID, TABLE_ID, str(copy_job.ended - copy_job.started)) )
#+end_src
** Check all columns for null
#+begin_src sql
SELECT col_name, COUNT(1) nulls_count
FROM `project.dataset.table` t,
UNNEST(REGEXP_EXTRACT_ALL(TO_JSON_STRING(t), r'"(\w+)":null')) col_name
GROUP BY col_name 
#+end_src
** Delimited column header
#+begin_src python
col_query = '''
select 
 col.column_name
from 
 `{0}.{1}.INFORMATION_SCHEMA.COLUMNS` col
where 
 col.table_name = '{2}'
order by 
 col.column_name
'''.format(PROJECT_ID, DATASET_ID, INPUT_TABLE_NAME)

COLUMN_LIST = pandas_gbq.read_gbq(col_query,project_id=PROJECT_ID,use_bqstorage_api=True)

HEADER_LIST = COLUMN_LIST.astype(str).values.flatten().tolist()
HEADER_STRING = '|'.join(HEADER_LIST)
print(HEADER_STRING)
#+end_src
* Instances
** Poweroff from Python
#+begin_src python
os.system("sudo poweroff")
#+end_src

* Dataflow
** SQL to TXT
#+begin_src python

from __future__ import division
import apache_beam as beam
from apache_beam.io import ReadFromBigQuery, WriteToText
from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions, WorkerOptions
import logging
######################################
#        Pipeline Configuration      #
######################################
#Set options for pipeline
pipeline_options = PipelineOptions(
      runner='DataflowRunner',
      project='PROJECT_ID',
      job_name='JOBNAME',
      region='us-east4',
      subnetwork= 'SUBNETWORK',
      use_public_ips=False,
      staging_location = 'gs://STAGING_LOCATION/',
      temp_location = 'gs://TEMP_LOCATION',
      machine_type = "n1-standard-1",
      max_num_workers = 30,
      autoscaling_algorithm='THROUGHPUT_BASED'#None
     )

#Set options for the file to be exported
DESTINATION_FILE_CONFIG = {
    'file_path_prefix' : 'gs://PATH_HERE'
    ,'file_name_suffix' : '.txt'
    ,'num_shards' : 1 # number of files to be created
    ,'header' : HEADER_STRING
   }


#ParDo class
class JoiningDoFn2(beam.DoFn):
    # Do lazy initializaiton here. Otherwise, error messages pop up, associated with "A large DoFn instance that is serialized for transmission to remote workers.""
    def __init__(self):
        import pandas as pd
	self.pd = pd
    def process(self,dic):
       return ['|'.join(str(x) for x in dic.values())]

 ######################################
 #               Pipeline             #
 ######################################

class DataFlowPipeline:
    """THIS IS THE CLASS THAT ACTUALLY RUNS THE JOB"""

    def run(self):
        """This is the job runner it holds the beam pipeline"""
        with beam.Pipeline(options=pipeline_options) as p:
	    pull_table_query =  """
	     select *
	     from 
	     {0}        
	    """.format(INPUT_TABLE_ID)

	#Driver averaging pipeline
	ent_modeling = p | 'read table' >> beam.io.Read(beam.io.ReadFromBigQuery(query=pull_table_query, use_standard_sql=True))  \
	                 | 'ParDo' >> beam.ParDo(JoiningDoFn2())  \
		         | 'Write Result to file' >> beam.io.WriteToText(**DESTINATION_FILE_CONFIG)

######################################
#              Main                  #
######################################
if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)
    print('setting up config for runner...')
    trainer = DataFlowPipeline()
    trainer.run()
    print('The runner is done!')
#+end_src
